#app/parsers/bank_parser.py
import asyncio
import re
from typing import Optional
from bs4 import BeautifulSoup
from playwright.async_api import async_playwright
import requests
import urllib3

urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)


class BankPageParser:
    """–ü–∞—Ä—Å–µ—Ä —Å—Ç—Ä–∞–Ω–∏—Ü –±–∞–Ω–∫–æ–≤ —Å —É–º–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–æ–π"""
    
    def __init__(self):
        self.headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
            "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8",
            "Accept-Language": "ru-RU,ru;q=0.9",
            "Accept-Encoding": "gzip, deflate",
            "DNT": "1",
            "Connection": "keep-alive",
            "Upgrade-Insecure-Requests": "1"
        }
        self.cache = {}
    
    async def get_page_content(self, url: str, max_retries: int = 3) -> Optional[str]:
        """
        ‚úÖ –ü–æ–ª—É—á–∞–µ—Ç –∫–æ–Ω—Ç–µ–Ω—Ç —Å—Ç—Ä–∞–Ω–∏—Ü—ã —Å –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã–º–∏ –ø–æ–ø—ã—Ç–∫–∞–º–∏
        
        –°—Ç—Ä–∞—Ç–µ–≥–∏—è:
        1. –ü–æ–ø—ã—Ç–∫–∞ —á–µ—Ä–µ–∑ requests (–±—ã—Å—Ç—Ä–æ)
        2. –ü–æ–ø—ã—Ç–∫–∞ —á–µ—Ä–µ–∑ Playwright (–¥–ª—è JS)
        3. –°–ø–µ—Ü–∏–∞–ª—å–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –¥–ª—è —Ä–∞–∑–Ω—ã—Ö –±–∞–Ω–∫–æ–≤
        """
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫—ç—à
        if url in self.cache:
            print(f"üíæ –ò—Å–ø–æ–ª—å–∑—É–µ–º –∫—ç—à –¥–ª—è {url}")
            return self.cache[url]
        
        # 1Ô∏è‚É£ –ü–æ–ø—ã—Ç–∫–∞ 1: –ë—ã—Å—Ç—Ä–∞—è –∑–∞–≥—Ä—É–∑–∫–∞ —á–µ—Ä–µ–∑ requests
        print(f"üîÑ –ü–æ–ø—ã—Ç–∫–∞ 1: requests –¥–ª—è {url}")
        content = await self._load_with_requests(url)
        if content and len(content) > 1000:
            print(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ —á–µ—Ä–µ–∑ requests")
            self.cache[url] = content
            return content
        
        # 2Ô∏è‚É£ –ü–æ–ø—ã—Ç–∫–∞ 2: Playwright –¥–ª—è JS
        print(f"üîÑ –ü–æ–ø—ã—Ç–∫–∞ 2: Playwright –¥–ª—è {url}")
        content = await self._load_with_playwright(url)
        if content and len(content) > 1000:
            print(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ —á–µ—Ä–µ–∑ Playwright")
            self.cache[url] = content
            return content
        
        # 3Ô∏è‚É£ –ü–æ–ø—ã—Ç–∫–∞ 3: –°–ø–µ—Ü–∏–∞–ª—å–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –ø–æ –¥–æ–º–µ–Ω–∞–º
        print(f"üîÑ –ü–æ–ø—ã—Ç–∫–∞ 3: –°–ø–µ—Ü–∏–∞–ª—å–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –¥–ª—è {url}")
        content = await self._load_with_special_handling(url)
        if content and len(content) > 1000:
            print(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ —Å–æ —Å–ø–µ—Ü–∏–∞–ª—å–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–æ–π")
            self.cache[url] = content
            return content
        
        print(f"‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å {url}")
        return None
    
    async def _load_with_requests(self, url: str) -> Optional[str]:
        """–ó–∞–≥—Ä—É–∑–∫–∞ —á–µ—Ä–µ–∑ requests"""
        try:
            response = requests.get(
                url,
                headers=self.headers,
                timeout=10,
                verify=False,
                allow_redirects=True
            )
            
            if response.status_code == 200:
                response.encoding = 'utf-8'
                return response.text
        except Exception as e:
            print(f"  ‚ö†Ô∏è requests –æ—à–∏–±–∫–∞: {type(e).__name__}")
        
        return None
    
    async def _load_with_playwright(self, url: str, timeout: int = 30000) -> Optional[str]:
        """–ó–∞–≥—Ä—É–∑–∫–∞ —á–µ—Ä–µ–∑ Playwright (–¥–ª—è JS –∫–æ–Ω—Ç–µ–Ω—Ç–∞)"""
        try:
            async with async_playwright() as p:
                browser = await p.chromium.launch(
                    headless=True,
                    args=[
                        '--disable-blink-features=AutomationControlled',
                        '--disable-dev-shm-usage',
                        '--no-sandbox',
                        '--disable-setuid-sandbox',
                    ]
                )
                
                context = await browser.new_context(
                    viewport={'width': 1920, 'height': 1080},
                    user_agent=self.headers['User-Agent']
                )
                
                page = await context.new_page()
                
                try:
                    # –ñ–¥—ë–º –∑–∞–≥—Ä—É–∑–∫–∏ —Å–µ—Ç–∏
                    await page.goto(url, wait_until='networkidle', timeout=timeout)
                    
                    # –ü—Ä–æ–∫—Ä—É—á–∏–≤–∞–µ–º –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏ –ª–µ–Ω–∏–≤–æ–≥–æ –∫–æ–Ω—Ç–µ–Ω—Ç–∞
                    await page.evaluate("""
                        async () => {
                            await new Promise((resolve) => {
                                let totalHeight = 0;
                                const distance = 100;
                                const timer = setInterval(() => {
                                    const scrollHeight = document.body.scrollHeight;
                                    window.scrollBy(0, distance);
                                    totalHeight += distance;
                                    
                                    if(totalHeight >= scrollHeight){
                                        clearInterval(timer);
                                        resolve();
                                    }
                                }, 100);
                            });
                        }
                    """)
                    
                    # –ü–æ–ª—É—á–∞–µ–º –∫–æ–Ω—Ç–µ–Ω—Ç
                    content = await page.content()
                    await browser.close()
                    return content
                    
                except Exception as e:
                    print(f"  ‚ö†Ô∏è Playwright –æ—à–∏–±–∫–∞: {type(e).__name__}")
                    await browser.close()
                    return None
                    
        except Exception as e:
            print(f"  ‚ö†Ô∏è –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞ Playwright: {type(e).__name__}")
            return None
    
    async def _load_with_special_handling(self, url: str) -> Optional[str]:
        """–°–ø–µ—Ü–∏–∞–ª—å–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –¥–ª—è —Ä–∞–∑–Ω—ã—Ö –±–∞–Ω–∫–æ–≤"""
        try:
            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –±–∞–Ω–∫ –ø–æ URL
            if "sberbank.by" in url or "sber" in url.lower():
                return await self._load_sberbank(url)
            elif "alfabank" in url.lower():
                return await self._load_alfabank(url)
            elif "mtbank" in url.lower():
                return await self._load_mtbank(url)
            else:
                # –°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–∞—è –∑–∞–≥—Ä—É–∑–∫–∞ —Å –¥—Ä—É–≥–∏–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏
                headers = self.headers.copy()
                headers['Referer'] = url.rsplit('/', 1)[0] + '/'
                
                response = requests.get(
                    url,
                    headers=headers,
                    timeout=15,
                    verify=False,
                    allow_redirects=True
                )
                
                if response.status_code == 200:
                    response.encoding = 'utf-8'
                    return response.text
        except Exception as e:
            print(f"  ‚ö†Ô∏è –°–ø–µ—Ü–∏–∞–ª—å–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –æ—à–∏–±–∫–∞: {type(e).__name__}")
        
        return None
    
    async def _load_sberbank(self, url: str) -> Optional[str]:
        """–°–ø–µ—Ü–∏–∞–ª—å–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –¥–ª—è –°–±–µ—Ä–∞"""
        try:
            headers = self.headers.copy()
            headers['Referer'] = 'https://www.sber-bank.by/'
            
            response = requests.get(url, headers=headers, timeout=12, verify=False)
            if response.status_code == 200:
                return response.text
        except Exception as e:
            print(f"  ‚ö†Ô∏è Sberbank –æ—à–∏–±–∫–∞: {type(e).__name__}")
        return None
    
    async def _load_alfabank(self, url: str) -> Optional[str]:
        """–°–ø–µ—Ü–∏–∞–ª—å–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –¥–ª—è –ê–ª—å—Ñ–∞ –ë–∞–Ω–∫–∞"""
        try:
            headers = self.headers.copy()
            headers['Referer'] = 'https://www.alfabank.by/'
            
            response = requests.get(url, headers=headers, timeout=12, verify=False)
            if response.status_code == 200:
                return response.text
        except Exception as e:
            print(f"  ‚ö†Ô∏è Alfabank –æ—à–∏–±–∫–∞: {type(e).__name__}")
        return None
    
    async def _load_mtbank(self, url: str) -> Optional[str]:
        """–°–ø–µ—Ü–∏–∞–ª—å–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –¥–ª—è –ú–¢–ë–∞–Ω–∫–∞"""
        try:
            headers = self.headers.copy()
            headers['Referer'] = 'https://www.mtbank.by/'
            
            response = requests.get(url, headers=headers, timeout=12, verify=False)
            if response.status_code == 200:
                return response.text
        except Exception as e:
            print(f"  ‚ö†Ô∏è MTBank –æ—à–∏–±–∫–∞: {type(e).__name__}")
        return None
    
    def extract_text(self, html: str, min_length: int = 100) -> str:
        """
        ‚úÖ –£–º–Ω–æ–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ –∏–∑ HTML
        - –£–¥–∞–ª—è–µ—Ç —Å–∫—Ä–∏–ø—Ç—ã –∏ —Å—Ç–∏–ª–∏
        - –°–æ—Ö—Ä–∞–Ω—è–µ—Ç —Å—Ç—Ä—É–∫—Ç—É—Ä—É
        - –û—á–∏—â–∞–µ—Ç –æ—Ç –ª–∏—à–Ω–∏—Ö –ø—Ä–æ–±–µ–ª–æ–≤
        """
        try:
            soup = BeautifulSoup(html, "html.parser")
            
            # –£–¥–∞–ª—è–µ–º –ª–∏—à–Ω–∏–µ —Ç–µ–≥–∏
            for tag in soup(['script', 'style', 'meta', 'link', 'svg', 'iframe', 
                           'noscript', 'nav', 'footer', 'button', 'form']):
                tag.decompose()
            
            # –£–¥–∞–ª—è–µ–º –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–∏
            for element in soup(string=lambda text: isinstance(text, str) and text.strip().startswith('<!--')):
                element.extract()
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ç–µ–∫—Å—Ç —Å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ–º —Å—Ç—Ä—É–∫—Ç—É—Ä—ã
            text = soup.get_text(separator=" ", strip=True)
            
            # –û—á–∏—â–∞–µ–º –æ—Ç –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö –ø—Ä–æ–±–µ–ª–æ–≤
            text = re.sub(r'\s+', ' ', text)
            
            # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º —Ä–∞–∑–º–µ—Ä (8000 —Å–∏–º–≤–æ–ª–æ–≤ –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–ª—è LLM)
            return text[:8000]
        
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è —Ç–µ–∫—Å—Ç–∞: {e}")
            return ""
    
    def extract_structured_data(self, html: str) -> dict:
        """
        ‚úÖ –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –∏–∑ HTML
        - –¢–∞–±–ª–∏—Ü—ã
        - –°–ø–∏—Å–∫–∏
        - –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è
        """
        try:
            soup = BeautifulSoup(html, "html.parser")
            data = {}
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ç–∞–±–ª–∏—Ü—ã
            tables = soup.find_all('table')
            if tables:
                data['tables'] = []
                for table in tables[:3]:  # –ü–µ—Ä–≤—ã–µ 3 —Ç–∞–±–ª–∏—Ü—ã
                    rows = []
                    for tr in table.find_all('tr')[:10]:  # –ü–µ—Ä–≤—ã–µ 10 —Å—Ç—Ä–æ–∫
                        cells = [td.get_text(strip=True) for td in tr.find_all(['td', 'th'])]
                        if cells:
                            rows.append(cells)
                    if rows:
                        data['tables'].append(rows)
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º —Å–ø–∏—Å–∫–∏
            lists = soup.find_all(['ul', 'ol'])
            if lists:
                data['lists'] = []
                for lst in lists[:5]:
                    items = [li.get_text(strip=True) for li in lst.find_all('li')]
                    if items:
                        data['lists'].append(items)
            
            return data
        
        except Exception as e:
            print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö: {e}")
            return {}


# –ì–ª–æ–±–∞–ª—å–Ω—ã–π —ç–∫–∑–µ–º–ø–ª—è—Ä –ø–∞—Ä—Å–µ—Ä–∞
parser = BankPageParser()


async def get_page_content(url: str) -> Optional[str]:
    return await parser.get_page_content(url)


async def extract_page_text(url: str) -> str:
    content = await get_page_content(url)
    if content:
        return parser.extract_text(content)
    return ""